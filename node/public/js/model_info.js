
function changeText(modelnumber) {
    var element = document.getElementById("modelP");
    switch (modelnumber) {
        case 'model1':
            element.innerHTML =
                "The model1 neural network is trained using a sigmoid activation function. The sigmoid function is a mathematical function that maps any real value to a value between 0 and 1. It introduces non-linearity to the network and is commonly used in the hidden layers of neural networks.<br><br>The training dataset used for model1 consists of 10,000 handwritten digits from the MNIST dataset. The MNIST dataset is a widely used benchmark dataset in the field of machine learning, consisting of 60,000 training images and 10,000 test images. Each image in the dataset represents a handwritten digit from 0 to 9. By utilizing 10,000 images from the dataset, model1 has access to a diverse set of handwritten digits to learn from.<br><br>During the training process, the data is divided into batches of size 10. Dividing the data into smaller batches allows for more efficient computation and memory usage. The network goes through each batch one by one and updates its weights and biases based on the gradients computed from that batch. This technique is known as mini-batch gradient descent and strikes a balance between accuracy and computational efficiency.<br><br>Model1 goes through one epoch during training, which means it processes the entire training dataset once. In each epoch, the network goes through each batch of images, calculates the loss (a measure of how different the predicted output is from the true output), and adjusts its parameters (weights and biases) based on the gradients of the loss with respect to those parameters. This process is repeated for each batch until all the batches have been processed, completing one epoch.<br><br>The 'Augment' parameter being set to False indicates that no data augmentation techniques were applied to the training dataset. Data augmentation is a common strategy used to artificially increase the size and diversity of the training dataset by applying various transformations, such as rotation, scaling, or flipping, to the images. These techniques help the model generalize better and reduce overfitting. However, in the case of model1, no such augmentation was used.<br><br>Model1 does not have any hidden layers, suggesting a simple architecture known as a single-layer perceptron or a fully connected neural network. In this architecture, the input layer directly connects to the output layer, with each input pixel connected to every output neuron. Despite its simplicity, model1 achieves a reported accuracy of 90.4% on the MNIST dataset. This accuracy indicates how well the model performs in correctly categorizing handwritten digits, demonstrating its effectiveness even with a straightforward architecture."
            break;
        case 'model2':
            element.innerHTML =
                "the ReLU activation function in the model2 neural network is a piecewise linear function that outputs the input directly if it is positive, and zero otherwise. This activation function is computationally efficient and has been widely adopted in deep learning models due to its simplicity and effectiveness.<br><br>The MNIST dataset used to train model2 is a well-known benchmark dataset in the field of machine learning. It consists of 60,000 training images and 10,000 test images, with each image representing a handwritten digit from 0 to 9. By using 10,000 images from the dataset, model2 has access to a diverse set of handwritten digits to learn from.<br><br>Dividing the data into batches of size 10 allows for more efficient computation during training. Instead of processing all the training examples at once, the network updates its weights and biases based on the gradients computed from each batch. This technique, known as mini-batch gradient descent, strikes a balance between accuracy and computational efficiency.<br><br>Training the network for one epoch means that the entire training dataset is used once during the training process. The network goes through each batch of images, calculates the loss, and updates its parameters based on the gradients. This process is repeated for each batch until all the batches have been processed, completing one epoch.<br><br>The 'Augment' parameter being set to False implies that no data augmentation techniques were applied to the training dataset. Data augmentation is a common strategy used to artificially increase the size of the training dataset by applying transformations such as rotations, translations, and flips to the images. This helps the model generalize better and reduces the risk of overfitting. However, in this case, model2 was trained without such augmentation.<br><br>With no hidden layers, model2 follows a simple architecture known as a single-layer perceptron or a fully connected neural network. The input layer directly connects to the output layer, where each input pixel is connected to every output neuron. Despite its simplicity, model2 achieves a reported accuracy of 95.8% on the MNIST dataset. This accuracy indicates that the model performs well in accurately categorizing handwritten digits, showcasing its effectiveness even with a straightforward architecture."
            break;
        case 'model3':
            element.innerHTML =
                "The model3 neural network is trained using the Rectified Linear Unit (ReLU) activation function. ReLU is a popular activation function that introduces non-linearity to the network. It outputs the input directly if it is positive, and zero otherwise. ReLU helps the network learn complex patterns and features from the input data, making it effective for deep learning models.<br><br>The training dataset for model3 is larger, consisting of 60,000 handwritten digits from the MNIST dataset. The MNIST dataset is a well-known benchmark dataset in the field of machine learning. It contains 60,000 training images and 10,000 test images. Each image represents a handwritten digit from 0 to 9. The larger dataset provides more training examples, allowing model3 to learn from a diverse set of handwritten digits.<br><br>During training, the data is divided into batches of size 10. Dividing the data into smaller batches improves computation efficiency and memory usage. The network processes each batch one by one, updating its weights and biases based on the gradients computed from that batch. Training the network for one epoch means it processes the entire training dataset once. In each epoch, the network goes through each batch, calculates the loss (a measure of the difference between predicted and true outputs), and adjusts its parameters (weights and biases) based on the gradients of the loss. This process is repeated for each batch until one epoch is completed.<br><br>The 'Augment' parameter being set to False indicates that no data augmentation techniques were applied during training. Data augmentation is a common strategy that artificially increases the size and diversity of the training dataset by applying various transformations, such as rotations, translations, and flips, to the images. However, in the case of model3, no augmentation techniques were used.<br><br>Model3 does not have any hidden layers, suggesting a simple architecture with only an input and an output layer. This architecture is known as a single-layer perceptron or a fully connected neural network. In this architecture, each input pixel is connected to every output neuron. Despite its simplicity, model3 achieves a reported accuracy of 98.7% on the MNIST dataset. This high accuracy indicates the model's excellent performance in accurately categorizing handwritten digits, showcasing its effectiveness even with a straightforward architecture."

            break;
        case 'model4':
            element.innerHTML =
                "The model4 neural network is trained using the Rectified Linear Unit (ReLU) activation function, which is similar to the activation functions used in the previous models. ReLU introduces non-linearity to the network and helps it learn complex patterns and features from the input data. Just like the previous models, the training dataset for model4 consists of 60,000 handwritten digits from the MNIST dataset. The data is divided into batches of size 10, and the network is trained for one epoch, meaning it processes the entire training dataset once during the training process. \
                <br><br>Similar to the previous models, no data augmentation techniques were applied during training as indicated by the 'Augment' parameter being set to False. Data augmentation techniques, such as rotation or scaling, are commonly used to artificially increase the size and diversity of the training dataset. However, in the case of model4, no such augmentation techniques were used.<br><br>Model4 introduces a hidden layer with 28 nodes, providing additional capacity and flexibility to the network. This hidden layer allows the network to learn more complex representations of the input data and capture higher-level features.<br><br>The reported accuracy of model4 is 98.4%, indicating its high performance in accurately categorizing handwritten digits. This accuracy metric measures how well the model predicts the correct digit category for the given inputs. With an accuracy of 98.4%, model4 demonstrates its effectiveness in accurately classifying handwritten digits, showcasing its high performance."
            break;
        case 'model5':
            element.innerHTML =
            'The model5 neural network is trained using the Rectified Linear Unit (ReLU) activation function, which introduces non-linearity and helps the network learn complex patterns and features from the input data. The training dataset for model5 consists of 60,000 handwritten digits from the MNIST dataset. The data is divided into batches of size 10, and the network is trained for one epoch, processing the entire training dataset once during training. The "Augment" parameter is set to False, indicating that no data augmentation techniques were applied during training. Model5 incorporates two hidden layers, the first with 512 nodes and the second with 256 nodes. These hidden layers provide additional capacity and flexibility, enabling the network to learn more complex representations of the input data. The reported accuracy of model5 is 98.1%, demonstrating its high performance in accurately categorizing handwritten digits.'
            break;
        case 'model6':
            element.innerHTML =
            'The model6 neural network is trained using the Rectified Linear Unit (ReLU) activation function, which allows the network to learn complex patterns and features from the input data by introducing non-linearity. For training, model6 utilizes a training dataset consisting of 60,000 handwritten digits from the MNIST dataset.<br><br> The dataset is divided into batches of size 10, and the network is trained for 10 epochs. In each epoch, the network processes the entire training dataset once, allowing it to learn from the complete set of handwritten digits. No data augmentation techniques were applied during training, as indicated by the "Augment" parameter being set to False. Model6 follows a simple architecture without any hidden layers, meaning there are no additional layers between the input and output layers. With a reported accuracy of 99.9%, model6 demonstrates exceptional performance in accurately categorizing handwritten digits.'
            break;
        case 'model7':
            element.innerHTML =
            "The model7 neural network is trained using the Rectified Linear Unit (ReLU) activation function, which introduces non-linearity and helps the network learn complex patterns and features from the input data. For training, model7 utilizes a training dataset consisting of 60,000 handwritten digits from the MNIST dataset. The dataset is divided into batches of size 10, and the network is trained for 10 epochs. Each epoch involves the network processing the entire training dataset once during training. No data augmentation techniques were applied during training, as indicated by the 'Augment' parameter being set to False.<br><br> Model7 includes two hidden layers, with the first hidden layer containing 512 nodes and the second hidden layer containing 256 nodes. These hidden layers provide additional capacity and flexibility to the network, allowing it to learn more complex representations of the input data. With a reported accuracy of 99.7%, model7 demonstrates high performance in accurately categorizing handwritten digits. This accuracy metric indicates the model's ability to correctly classify the given inputs, highlighting its effectiveness in capturing the underlying patterns and features of the handwritten digit data."
            break;
    }

}