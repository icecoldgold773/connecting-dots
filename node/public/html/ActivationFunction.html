<!DOCTYPE html>
<html>
<head>
    <link
    rel="favcon"
    type="image/png"
    sizes="180x180"
    href="../img/favcon.png" />
<link
    rel="icon"
    type="image/png"
    sizes="32x32"
    href="../img/favcon.png" />
<link
    rel="icon"
    type="image/png"
    sizes="16x16"
    href="../img/favicon-16x16.png" />
<!--basic html stuff-->
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!--languages-->
<meta name="language" content="English" />
<!--CSS Stylesheets -->
<link rel="stylesheet" type="text/css" href="../css/header.css" />
<link rel="stylesheet" type="text/css" href="../css/footer.css" />
<link rel="stylesheet" type="text/css" href="../css/base.css" />
<link rel="stylesheet" type="text/css" href="../css/infosider.css" />
</head>

<body>
    <!--Navigation-bar-->
    <a name="home-point"></a> 
		<div class="topnav" id="myTopnav">
			<div href="../" class="divLogo">
				<button href="../"  class="homebutton">
				<a class="sub-nav-link is-active" href="../">
					<img href="../"  class="logo" src="../img/NN.png" />
				</a>
			</button>
			</div>
			<div class="container">
				<div class="navbar_title">
					<button class="navbar_button" onclick="javascript:window.location.href='./NeuralNetwork.html'">Neural Networks</button>
				</div>
				<div class="navbar_title">
					<button class="navbar_button" onclick="javascript:window.location.href='./MultiLayerPerceptron.html'">Cost functions</button>
				</div>
				<div class="navbar_title">
					<button class="navbar_button_active" onclick="javascript:window.location.href='./ActivationFunction.html'">Activation Functions</button>
				</div>
				<div class="navbar_title">
					<button class="navbar_button" onclick="javascript:window.location.href='./GradientDescent.html'">Gradient Descent</button>
				</div>
				<div class="navbar_title">
					<button class="navbar_button" onclick="javascript:window.location.href='./Backpropagation.html'">Back-propagation</button>
				</div>
				<div class="navbar_title">
					<button class="navbar_button" onclick="javascript:window.location.href='./FeatureMaps.html'">Feature maps</button>
				</div>
			</div>
		</div>

<!--Main COntent-->
<div class="container_main_content">
<div class="main_content">
  <h1>What are Activation Functions?</h1>
  <p><b>An activation function</b> can be seen as deciding whether a neuron should be activated or not, or how much its input should weight in the network. The role of the activation function is to compute an output from a set of input values for the neuron.</p><br>
  <p>Some commonly used activation functions are <b>sigmoid</b>, <b>hyperbolic
    tangent</b> (Tanh) and <b>ReLu</b> for binary classification and <b>softmax</b> for multi-class classification.</p><br>

  <div class="top_bar">
  <h2 class="top_bar_heading">Sigmoid</h2>   
  <p>The sigmoid function is a non-linear function that maps the input to a value between 0 and 1. The sigmoid is commonly employed in neural networks because its derivative is simple to calculate and computationally quick, also making it ideal for backpropagation, which is the neural networks training process, <a class="link" href="./Backpropagation.html">read about backpropagation here</a></p>
  <figure>
  <img class="smallfigure" src="../img/sigmoid.png"/>
  <figcaption>Graph of the sigmoid function</figcaption>
  </figure><br>
  </div>

  <div class="top_bar">
  <h2>Hyperbolic Tangent</h2> 
  <p>The Tanh function is a non-linear function that maps the input to a value between -1 and 1.</p>
  <figure>
  <img class="smallfigure" src="../img//hyperbolic.png"/>
  <figcaption>Graph of the Tanh function</figcaption>
  </figure><br>
  </div>

  <div class="top_bar">
  <h2>Rectified Linear Unit</h2> 
  <p>The ReLu function is a piecewise linear function that blocks negative values and maps positive values to themselves. The advantage of the ReLu function is that not all the neurons are activated simultaneously, which makes the network more efficient.</p>
  <figure>
  <img class="smallfigure" src="../img/ReLu.png"/>
  <figcaption>Graph of the ReLu function</figcaption>
  </figure><br>
  </div>

  <div class="top_bar">
    <h2>Softmax</h2> 
    <p>The softmax function is a multiclass activation function which means that it can be used to classify the output of a problem with more than two outcomes. The softmax function is a combination of sigmoid functions and it compresses the output of the neural network into the range [0, 1].</p>
    <figure>
    <img class="smallfigure" src="../img/leakyReLu.png"/>
    <figcaption>Graph of the softmax function</figcaption>
    </figure><br>
    </div>

</div>
</div>
</body>

</html>